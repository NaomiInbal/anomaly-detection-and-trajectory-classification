{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM+kxl9Z1/5U2Mjk0oChxp7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NaomiInbal/anomaly-detection-and-trajectory-classification/blob/missile_trajectories_for_final_report/LSTM_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM MODELS\n",
        "This code uploads and runs models of data of missile trajectories in the folder called \"missile_data\".\n",
        "\n",
        "To run the models, you must first run the simulator that creates the data.This is the link to the simulator:\n",
        "https://colab.research.google.com/drive/1ig9DJSc9_JOm16icqWhLN5DHBLDNVAjN#scrollTo=gVCcUOTU-1gr\n",
        "\n",
        "Alternatively, you can download the existing data from this link: https://drive.google.com/drive/folders/1vsQ1DBiNDY6Ene-2mJa-0BbeGeTBWpT4?usp=drive_link\n",
        "and put it in Google Drive storage at the location: /content/gdrive/My Drive/\n",
        "\n",
        "In addition, you will be asked to allow access to Google Drive to upload the data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7w-lim1EGYL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "WBLLHWJZxH9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Upload Data"
      ],
      "metadata": {
        "id": "YrdHsVFZ0YL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "#drive.mount('/content/gdrive')\n",
        "drive.mount(\"/content/gdrive\", force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3eTIxMaQ2qS",
        "outputId": "83ddf64b-d67e-4ad0-9026-37238895f180"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "_5qynjaMwRAr"
      },
      "outputs": [],
      "source": [
        "def upload_data(drive_folder=\"missile_data\"):\n",
        "    \"\"\"\n",
        "    Load dataset from CSV files in a specified folder on Google Drive.\n",
        "\n",
        "    Parameters:\n",
        "    - drive_folder (str): Folder name in Google Drive where the dataset is stored.\n",
        "\n",
        "    Returns:\n",
        "    - X (numpy.ndarray): Array containing input data from CSV files.\n",
        "    - Y (numpy.ndarray): Array containing labels from CSV files.\n",
        "    \"\"\"\n",
        "    print(\"\"\"Starts by upload  missile trajectories from Google Drive...\n",
        "    This is going to take a while....\"\"\")\n",
        "    # Construct the full path to the dataset folder in Google Drive\n",
        "    data_dir = f\"/content/gdrive/My Drive/{drive_folder}\"\n",
        "\n",
        "    # Check if the folder exists\n",
        "    if not os.path.exists(data_dir):\n",
        "        print(f\"Folder '{data_dir}' does not exist.\")\n",
        "        print(\"Upload failed!\")\n",
        "        return None, None\n",
        "\n",
        "    # List all CSV files in the specified folder\n",
        "    files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]\n",
        "\n",
        "    # Print the number of CSV files\n",
        "    print(f\"There are {len(files)} CSV files in the '{drive_folder}' folder.\")\n",
        "\n",
        "    # Create empty lists to store data from each CSV file\n",
        "    X_list = []\n",
        "    Y_list = []\n",
        "    required_columns = [\n",
        "    'Velocity[m/s]', 'Altitude[m]', 'Thrust[N]', 'gravitation [m/s^2]',\n",
        "    'Temperature[K]', 'Pressure[PSI]', 'Drag[N]', 'Mass[Kg]', 'Density',\n",
        "    'Label'\n",
        "    ]\n",
        "\n",
        "\n",
        "    # Load each CSV file into a separate DataFrame and append it to the lists\n",
        "    for file in files:\n",
        "        try:\n",
        "            df = pd.read_csv(os.path.join(data_dir, file))\n",
        "        except pd.errors.EmptyDataError:\n",
        "            print(f\"File '{file}' is empty.\")\n",
        "\n",
        "        # Check if the DataFrame has all required columns\n",
        "        if not all(col in df.columns for col in required_columns):\n",
        "            print(f\"File '{file}' does not include all required columns.\")\n",
        "\n",
        "        num_rows_in_data = 5001 #5000 somples + 1 for the titles\n",
        "        num_columns_in_data = 10\n",
        "        # Check if the data upload correctly\n",
        "        if df.shape != (num_rows_in_data, num_columns_in_data):\n",
        "            print(f\"File '{file}' does not meet the criteria: Found shape {df.shape}, expected {num_rows_in_data, num_columns_in_data}.\")\n",
        "            print(\"Upload failed!\")\n",
        "            return None, None\n",
        "        if 'Label' not in df.columns:\n",
        "            print(\"Label' not in columns\")\n",
        "            print(\"Upload failed!\")\n",
        "            return None, None\n",
        "        X_list.append(df.drop(columns=['Label']))\n",
        "        Y_list.append(df['Label'][0])\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Convert the lists of arrays to NumPy arrays\n",
        "        X = np.array(X_list)\n",
        "        Y = np.array(Y_list)\n",
        "\n",
        "        print(\"Upload finished successfully!\")\n",
        "        return X, Y\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during upload: {e}\")\n",
        "        print(\"Upload failed!\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing The Data"
      ],
      "metadata": {
        "id": "RMuszJtQ0fDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This function normalize the data using min max method\n",
        "def min_max_normalization(data):\n",
        "    min_val = np.min(data, axis=0)\n",
        "    max_val = np.max(data, axis=0)\n",
        "    normalized_data = ((data - min_val) / ((max_val - min_val) + 1e-7))\n",
        "    return normalized_data\n",
        "\n",
        "\n",
        "def encoder(x, y):\n",
        "    \"\"\"\n",
        "    This function splits the data into training set and validation set.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Features\n",
        "    - y: Labels\n",
        "\n",
        "    Returns:\n",
        "    - X_train: Training set features\n",
        "    - X_test: Validation set features\n",
        "    - y_train: Training set labels\n",
        "    - y_test: Validation set labels\n",
        "    \"\"\"\n",
        "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,\n",
        "                                                        random_state=42, stratify=y, shuffle=True)\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "Lo09QS4a0ifq"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auxiliary functions for LSTM models"
      ],
      "metadata": {
        "id": "Vv-4wqYx1Fjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.src.layers import BatchNormalization\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "import ast\n",
        "import tensorflow as tf\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import csv\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.optimizers import Adagrad\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "UfRlsrMv1GxQ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def callbacks_function(name):\n",
        "    \"\"\"\n",
        "    Generate and return a set of Keras callbacks commonly used in training neural networks.\n",
        "\n",
        "    Parameters:\n",
        "    - name (str): Name of the ModelCheckpoint file to save the best model weights.\n",
        "\n",
        "    Returns:\n",
        "    - tf.keras.callbacks.LearningRateScheduler: Learning rate scheduler that decreases the learning rate\n",
        "      by half every 5 epochs.\n",
        "    - tf.keras.callbacks.EarlyStopping: Callback to stop training when a monitored metric has stopped improving.\n",
        "    - tf.keras.callbacks.ModelCheckpoint: Callback to save the model weights during training.\n",
        "    \"\"\"\n",
        "    early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='min',\n",
        "                                                  restore_best_weights=True)\n",
        "    monitor = tf.keras.callbacks.ModelCheckpoint(name, monitor='val_loss', verbose=0, save_best_only=False\n",
        "                                                 , save_weights_only=False, mode='min')\n",
        "\n",
        "    # This functuon decrease the value of learning rate\n",
        "    def scheduler(epoch, lr):\n",
        "        if epoch % 5 == 0 and epoch > 0:\n",
        "            lr = lr / 2\n",
        "        return lr\n",
        "\n",
        "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=0)\n",
        "\n",
        "    return lr_schedule\n",
        "\n",
        "    lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=0)\n",
        "    return early_stop, monitor, lr_schedule\n",
        "\n",
        "\n",
        "# This function calculates the f1 values\n",
        "def f1(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the F1 score between binary true and predicted labels.\n",
        "\n",
        "    Parameters:\n",
        "    - y_true (tf.Tensor): True binary labels.\n",
        "    - y_pred (tf.Tensor): Predicted binary labels.\n",
        "\n",
        "    Returns:\n",
        "    - tf.Tensor: F1 score.\n",
        "\n",
        "    - F1 score is the harmonic mean of precision and recall.\n",
        "    - Precision and recall are calculated based on true positives, predicted positives, and possible positives.\n",
        "    - The parameter 'b' controls the weight of precision in the F1 score calculation.\n",
        "      \"\"\"\n",
        "    # This function calculates the recall\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + tf.keras.backend.epsilon())\n",
        "        return recall\n",
        "\n",
        "    # This function calculates the precision\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = tf.keras.backend.sum(tf.keras.backend.round(tf.keras.backend.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + tf.keras.backend.epsilon())\n",
        "        return precision\n",
        "\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    b = 0.5\n",
        "    return ((((1 + b) ** 2) * (precision * recall)) / (((b ** 2) * (precision)) + recall + tf.keras.backend.epsilon()))\n",
        "\n",
        "\n",
        "# This function uses to compile the models\n",
        "def model_comiple_run(num_epochs, initial_learning_rate, model, X_train, Y_train, X_test, y_test, callbacks,\n",
        "                      optimizer=\"Adam\"):\n",
        "    \"\"\"\n",
        "    Compile and train a neural network model using the specified parameters.\n",
        "\n",
        "    Parameters:\n",
        "    - num_epochs (int): Number of training epochs.\n",
        "    - initial_learning_rate (float): Initial learning rate for the optimizer.\n",
        "    - model (tf.keras.Model): Neural network model to be compiled and trained.\n",
        "    - X_train (tf.Tensor): Training input data.\n",
        "    - Y_train (tf.Tensor): Training target data.\n",
        "    - X_test (tf.Tensor): Testing input data.\n",
        "    - y_test (tf.Tensor): Testing target data.\n",
        "    - callbacks (tuple): Tuple of Keras callbacks to be used during training.\n",
        "    - optimizer (str): String specifying the optimizer to be used (default is \"Adam\").\n",
        "\n",
        "    Returns:\n",
        "    - tf.keras.callbacks.History: Object containing training metrics and loss history.\n",
        "    \"\"\"\n",
        "    opt = Adam(learning_rate=initial_learning_rate)\n",
        "    if (optimizer == \"Adagrade\"):\n",
        "        opt = Adagrad(learning_rate=initial_learning_rate)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy', f1])\n",
        "    model_history = model.fit(X_train, Y_train, validation_data=(X_test, y_test), verbose=1, epochs=num_epochs)\n",
        "    # batch_size=32, validation_data=(X_test,y_test)\\,callbacks=callbacks,verbose=1)\n",
        "    return model_history\n"
      ],
      "metadata": {
        "id": "8PUxp4qm1Lje"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def model_plot(model_history, type=None, plot_all=True):\n",
        "    \"\"\"\n",
        "    Plot training metrics and loss from a model's training history.\n",
        "\n",
        "    Parameters:\n",
        "    - model_history (tf.keras.callbacks.History): Training history obtained from model training.\n",
        "    - type (str): Type of plot to display ('accuracy', 'loss', 'f1'). Default is None.\n",
        "    - plot_all (bool): If True, plots all available metrics and losses. If False, only plots the specified type.\n",
        "    \"\"\"\n",
        "    plt.plot()\n",
        "    if plot_all:\n",
        "        plt.plot(model_history.history['accuracy'], label=\"Train accuracy\")\n",
        "        plt.plot(model_history.history['val_accuracy'], label=\"Val accuracy\")\n",
        "        plt.xlabel(\"Epoch (iteration)\")\n",
        "        plt.ylabel(\"Accuracy\")\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        plt.plot(model_history.history['loss'], label=\"Train loss\")\n",
        "        plt.plot(model_history.history['val_loss'], label=\"Val loss\")\n",
        "        plt.xlabel(\"Epoch (iteration)\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "        plt.plot(model_history.history['f1'], label=\"Train f1\")\n",
        "        plt.plot(model_history.history['val_f1'], label=\"Val f1\")\n",
        "        plt.xlabel(\"Epoch (iteration)\")\n",
        "        plt.ylabel(\"F1\")\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    else:\n",
        "        if type == 'accuracy':\n",
        "            plt.plot(model_history.history['accuracy'], label=\"Train accuracy\")\n",
        "            plt.plot(model_history.history['val_accuracy'], label=\"val_accuracy\")\n",
        "        if type == 'loss':\n",
        "            plt.plot(model_history.history['loss'], label=\"Train loss\")\n",
        "            plt.plot(model_history.history['val_loss'], label=\"val_loss\")\n",
        "        if type == 'f1':\n",
        "            # Correct the keys for F1 score\n",
        "            plt.plot(model_history.history['f1'], label=\"Train f1\")\n",
        "            plt.plot(model_history.history['val_f1'], label=\"val_f1\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cf_matrix):\n",
        "    \"\"\"\n",
        "    Plot a confusion matrix using a heatmap.\n",
        "\n",
        "    Parameters:\n",
        "    - cf_matrix (numpy.ndarray): Confusion matrix to be plotted.\n",
        "\n",
        "    - The function uses Seaborn and Matplotlib for plotting the confusion matrix heatmap.\n",
        "    - The confusion matrix is a 2x2 numpy array.\n",
        "    \"\"\"\n",
        "    print('Confusion Matrix')\n",
        "    ax = plt.subplot()\n",
        "    sns.heatmap(cf_matrix, annot=True, ax=ax, fmt='d', cmap='Blues', cbar=False)\n",
        "    # labels, title and ticks\n",
        "    ax.set_title('Confusion Matrix')\n",
        "    ax.set_xlabel('Predicted labels')\n",
        "    ax.set_ylabel('Actual labels')\n",
        "    ax.xaxis.set_ticklabels(['Not accident', 'Accident'])\n",
        "    ax.yaxis.set_ticklabels(['Not accident', 'Accident'])\n",
        "    plt.show()  # Display the plot\n"
      ],
      "metadata": {
        "id": "k0vNt5P61WkJ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Initial improvements to LSTM models\n",
        "These are initial attempts at running LSTM models. These are attempts that are not documented in the final report of the project, but they helped us understand and later write more advanced models with more suitable parameters. The more advanced models are detailed in the final report and appear here below under the heading: \"LSTM Models\""
      ],
      "metadata": {
        "id": "DPKt_hNj1b_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM - model 01 : hidden_layers = 1, total_nodes = 40, initial_learning_rate = 0.1, num_epochs = 100 ,optimizer='adam'\n",
        "def lstm_model01(X_train, X_test, y_train, y_test):\n",
        "    model = Sequential()\n",
        "    # input_shape - we define an LSTM model with an input shape of (param1, param2), meaning it takes in a sequence\n",
        "    # of param1 inputs with param2 feature each.\n",
        "    model.add(LSTM(40, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Flatten())\n",
        "    # The sigmoid activation function is commonly used for binary classification problems, where the output values\n",
        "    # range between 0 and 1\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    model_name = 'Model01'\n",
        "    lr_schedule = callbacks_function(model_name)\n",
        "    initial_learning_rate = 0.1\n",
        "    num_epochs = 100\n",
        "    model_history = model_comiple_run(num_epochs, initial_learning_rate, model, X_train, y_train, X_test, y_test,\n",
        "                                      callbacks=[lr_schedule])\n",
        "    model_plot(model_history)\n",
        "    loss = model.evaluate(X_test, y_test, verbose=1, steps=X_test.shape[0])  # evaluate the model\n",
        "    print('Final loss 1 (cross-entropy and accuracy and F1):', loss)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    # Define a threshold for binary classification (e.g., 0.5)\n",
        "    threshold = 0.5\n",
        "    y_pred_binary = (y_pred >= threshold).astype(int)  # Convert predicted probabilities to binary labels\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    plot_confusion_matrix(cf_matrix)\n",
        "    # Calculate precision\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    # Calculate recall\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "\n",
        "\n",
        "# LSTM - model 02 : hidden_layers = 1, initial_learning_rate = 0.001, optimizer='adam', num_epochs = 100\n",
        "# Improvement : DataBase - the small DB ,total_nodes = 128\n",
        "def lstm_model02(X_train, X_test, y_train, y_test):\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = Sequential()\n",
        "    # input_shape - we define an LSTM model with an input shape of (param1, param2), meaning it takes in a sequence\n",
        "    # of param1 inputs with param2 feature each.\n",
        "    model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    # model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    model_name = 'Model02'\n",
        "    lr_schedule = callbacks_function(model_name)\n",
        "    initial_learning_rate = 0.001\n",
        "    num_epochs = 100\n",
        "    model_history = model_comiple_run(num_epochs, initial_learning_rate, model, X_train, y_train, X_test, y_test,\n",
        "                                      callbacks=[lr_schedule])\n",
        "    model_plot(model_history)\n",
        "    loss = model.evaluate(X_test, y_test, verbose=1, steps=X_test.shape[0])  # evaluate the model\n",
        "    print('Final loss 1 (cross-entropy and accuracy and F1):', loss)\n",
        "    y_pred = model.predict(X_test)\n",
        "    threshold = 0.5  # Define a threshold for binary classification (e.g., 0.5)\n",
        "    y_pred_binary = (y_pred >= threshold).astype(int)  # Convert predicted probabilities to binary labels\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    plot_confusion_matrix(cf_matrix)\n",
        "    # Calculate precision & recall\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "\n",
        "\n",
        "# LSTM - model 03:  total_nodes = 128, initial_learning_rate = 0.001, num_epochs = 100, , optimizer='adam'\n",
        "# Improvement : hidden_layers = 2 (Adding one more lstm layer), DataBase - the big DB\n",
        "def lstm_model03(X_train, X_test, y_train, y_test):\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = Sequential()\n",
        "    # input_shape - we define an LSTM model with an input shape of (param1, param2), meaning it takes in a sequence\n",
        "    # of param1 inputs with param2 feature each.\n",
        "    model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "    model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    # model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    model_name = 'Model03'\n",
        "    lr_schedule = callbacks_function(model_name)\n",
        "    initial_learning_rate = 0.001\n",
        "    num_epochs = 100\n",
        "    model_history = model_comiple_run(num_epochs, initial_learning_rate, model, X_train, y_train, X_test, y_test,\n",
        "                                      callbacks=[lr_schedule])\n",
        "    model_plot(model_history)\n",
        "    loss = model.evaluate(X_test, y_test, verbose=1, steps=X_test.shape[0])  # evaluate the model\n",
        "    print('Final loss 1 (cross-entropy and accuracy and F1):', loss)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    threshold = 0.5  # Define a threshold for binary classification (e.g., 0.5)\n",
        "    y_pred_binary = (y_pred >= threshold).astype(int)  # Convert predicted probabilities to binary labels\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    plot_confusion_matrix(cf_matrix)\n",
        "    # Calculate precision & recall\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "\n",
        "\n",
        "# LSTM - model 04: hidden_layers = 2, total_nodes = 128, initial_learning_rate = 0.001, num_epochs = 100\n",
        "# Improvement : optimizer='Adagrade'\n",
        "def lstm_model04(X_train, X_test, y_train, y_test):\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = Sequential()\n",
        "    # input_shape - we define an LSTM model with an input shape of (param1, param2), meaning it takes in a sequence\n",
        "    # of param1 inputs with param2 feature each.\n",
        "    model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "    model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    model_name = 'Model04'\n",
        "    lr_schedule = callbacks_function(model_name)\n",
        "    initial_learning_rate = 0.001\n",
        "    num_epochs = 100\n",
        "    model_history = model_comiple_run(num_epochs, initial_learning_rate, model, X_train, y_train, X_test, y_test,\n",
        "                                      callbacks=[lr_schedule], optimizer=\"Adagrade\")\n",
        "    model_plot(model_history)\n",
        "    loss = model.evaluate(X_test, y_test, verbose=1, steps=X_test.shape[0])  # evaluate the model\n",
        "    print('Final loss 1 (cross-entropy and accuracy and F1):', loss)\n",
        "    y_pred = model.predict(X_test)\n",
        "    threshold = 0.5  # Define a threshold for binary classification (e.g., 0.5)\n",
        "    y_pred_binary = (y_pred >= threshold).astype(int)  # Convert predicted probabilities to binary labels\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    plot_confusion_matrix(cf_matrix)\n",
        "    # Calculate precision & recall\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "\n",
        "\n",
        "# LSTM - model 05: hidden_layers = 2, total_nodes = 128, initial_learning_rate = 0.001, num_epochs = 100, optimizer='adam'\n",
        "# Improvement : threshold = 0.3\n",
        "def lstm_model05(X_train, X_test, y_train, y_test):\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = Sequential()\n",
        "    # input_shape - we define an LSTM model with an input shape of (param1, param2), meaning it takes in a sequence of param1 inputs with param2 feature each.\n",
        "    model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "    model.add(LSTM(128, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    model_name = 'Model05'\n",
        "    lr_schedule = callbacks_function(model_name)\n",
        "    initial_learning_rate = 0.001\n",
        "    num_epochs = 100\n",
        "    model_history = model_comiple_run(num_epochs, initial_learning_rate, model, X_train, y_train, X_test, y_test,\n",
        "                                      callbacks=[lr_schedule])\n",
        "    model_plot(model_history)\n",
        "    loss = model.evaluate(X_test, y_test, verbose=1, steps=X_test.shape[0])  # evaluate the model\n",
        "    print('Final loss 1 (cross-entropy and accuracy and F1):', loss)\n",
        "    y_pred = model.predict(X_test)\n",
        "    threshold = 0.3  # Define a threshold for binary classification (e.g., 0.5)\n",
        "    y_pred_binary = (y_pred >= threshold).astype(int)  # Convert predicted probabilities to binary labels\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    plot_confusion_matrix(cf_matrix)\n",
        "    # Calculate precision & recall\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "\n",
        "# LSTM - model 06: hidden_layers = 2, num_epochs = 100\n",
        "# Improvement : optimizer='Adagrade', initial_learning_rate = 0.01, total_nodes = 16\n",
        "def lstm_model06(X_train, X_test, y_train, y_test):\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = Sequential()\n",
        "    # input_shape - we define an LSTM model with an input shape of (param1, param2), meaning it takes in a sequence\n",
        "    # of param1 inputs with param2 feature each.\n",
        "    model.add(LSTM(16, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "    model.add(LSTM(16, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    model_name = 'Model06'\n",
        "    lr_schedule = callbacks_function(model_name)\n",
        "    initial_learning_rate = 0.01\n",
        "    num_epochs = 100\n",
        "    model_history = model_comiple_run(num_epochs, initial_learning_rate, model, X_train, y_train, X_test, y_test,\n",
        "                                      callbacks=[lr_schedule], optimizer=\"Adagrade\")\n",
        "    model_plot(model_history)\n",
        "    loss = model.evaluate(X_test, y_test, verbose=1, steps=X_test.shape[0])  # evaluate the model\n",
        "    print('Final loss 1 (cross-entropy and accuracy and F1):', loss)\n",
        "    y_pred = model.predict(X_test)\n",
        "    threshold = 0.5\n",
        "    y_pred_binary = (y_pred >= threshold).astype(int)  # Convert predicted probabilities to binary labels\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    plot_confusion_matrix(cf_matrix)\n",
        "    # Calculate precision & recall\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "\n",
        "# LSTM - model 07 : hidden_layers = 1, initial_learning_rate = 0.1, num_epochs = 100, ,optimizer='adam'\n",
        "# Improvement : total_nodes = 1\n",
        "def lstm_model07(X_train, X_test, y_train, y_test):\n",
        "    model = Sequential()\n",
        "    # input_shape - we define an LSTM model with an input shape of (param1, param2), meaning it takes in a sequence\n",
        "    # of param1 inputs with param2 feature each.\n",
        "    model.add(LSTM(1, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    # model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "    # The sigmoid activation function is commonly used for binary classification problems, where the output values\n",
        "    # range between 0 and 1\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    model_name = 'Model07'\n",
        "    lr_schedule = callbacks_function(model_name)\n",
        "    initial_learning_rate = 0.1\n",
        "    num_epochs = 100\n",
        "    model_history = model_comiple_run(num_epochs, initial_learning_rate, model, X_train, y_train, X_test, y_test,\n",
        "                                      callbacks=[lr_schedule])\n",
        "    model_plot(model_history)\n",
        "    loss = model.evaluate(X_test, y_test, verbose=1, steps=X_test.shape[0])  # evaluate the model\n",
        "    print('Final loss 1 (cross-entropy and accuracy and F1):', loss)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    # Define a threshold for binary classification (e.g., 0.5)\n",
        "    threshold = 0.5\n",
        "    y_pred_binary = (y_pred >= threshold).astype(int)  # Convert predicted probabilities to binary labels\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    plot_confusion_matrix(cf_matrix)\n",
        "    # Calculate precision\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    # Calculate recall\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "\n",
        "# LSTM - model 08 : hidden_layers = 1, initial_learning_rate = 0.1, num_epochs = 100, ,optimizer='adam'\n",
        "# Improvement : total_nodes = 16\n",
        "def lstm_model08(X_train, X_test, y_train, y_test):\n",
        "    model = Sequential()\n",
        "    # input_shape - we define an LSTM model with an input shape of (param1, param2), meaning it takes in a sequence\n",
        "    # of param1 inputs with param2 feature each.\n",
        "    model.add(LSTM(16, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    # model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "    # The sigmoid activation function is commonly used for binary classification problems, where the output values\n",
        "    # range between 0 and 1\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    model_name = 'Model08'\n",
        "    lr_schedule = callbacks_function(model_name)\n",
        "    initial_learning_rate = 0.1\n",
        "    num_epochs = 100\n",
        "    model_history = model_comiple_run(num_epochs, initial_learning_rate, model, X_train, y_train, X_test, y_test,\n",
        "                                      callbacks=[lr_schedule])\n",
        "    model_plot(model_history)\n",
        "    loss = model.evaluate(X_test, y_test, verbose=1, steps=X_test.shape[0])  # evaluate the model\n",
        "    print('Final loss 1 (cross-entropy and accuracy and F1):', loss)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    # Define a threshold for binary classification (e.g., 0.5)\n",
        "    threshold = 0.5\n",
        "    y_pred_binary = (y_pred >= threshold).astype(int)  # Convert predicted probabilities to binary labels\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    plot_confusion_matrix(cf_matrix)\n",
        "    # Calculate precision\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    # Calculate recall\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "\n",
        "# LSTM - model 09: hidden_layers = 2, total_nodes = 16, optimizer='Adagrade', initial_learning_rate = 0.01, num_epochs = 50\n",
        "# Improvement : adding dropout\n",
        "def lstm_model09(X_train, X_test, y_train, y_test):\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = Sequential()\n",
        "    # input_shape - we define an LSTM model with an input shape of (param1, param2), meaning it takes in a sequence\n",
        "    # of param1 inputs with param2 feature each.\n",
        "    model.add(LSTM(16, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(LSTM(16, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    model_name = 'Model09'\n",
        "    lr_schedule = callbacks_function(model_name)\n",
        "    initial_learning_rate = 0.01\n",
        "    num_epochs = 50\n",
        "    model_history = model_comiple_run(num_epochs, initial_learning_rate, model, X_train, y_train, X_test, y_test,\n",
        "                                      callbacks=[lr_schedule], optimizer=\"Adagrade\")\n",
        "    model_plot(model_history)\n",
        "    loss = model.evaluate(X_test, y_test, verbose=1, steps=X_test.shape[0])  # evaluate the model\n",
        "    print('Final loss 1 (cross-entropy and accuracy and F1):', loss)\n",
        "    y_pred = model.predict(X_test)\n",
        "    threshold = 0.5\n",
        "    y_pred_binary = (y_pred >= threshold).astype(int)  # Convert predicted probabilities to binary labels\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    plot_confusion_matrix(cf_matrix)\n",
        "    # Calculate precision & recall\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n",
        "\n",
        "\n",
        "# LSTM - model 010 : hidden_layers = 1, total_nodes = 15, initial_learning_rate = 0.001 ,optimizer='adam'\n",
        "# Improvement : initial_learning_rate = 0.001, num_epochs = 20,  threshold = 0.3\n",
        "def lstm_model010(X_train, X_test, y_train, y_test):\n",
        "    model = Sequential()\n",
        "    # input_shape - we define an LSTM model with an input shape of (param1, param2), meaning it takes in a sequence\n",
        "    # of param1 inputs with param2 feature each.\n",
        "    model.add(LSTM(15, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    # model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "    # The sigmoid activation function is commonly used for binary classification problems, where the output values\n",
        "    # range between 0 and 1\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    model_name = 'Model010'\n",
        "    lr_schedule = callbacks_function(model_name)\n",
        "    initial_learning_rate = 0.001\n",
        "    num_epochs = 50\n",
        "    model_history = model_comiple_run(num_epochs, initial_learning_rate, model, X_train, y_train, X_test, y_test,\n",
        "                                      callbacks=[lr_schedule])\n",
        "    model_plot(model_history)\n",
        "    loss = model.evaluate(X_test, y_test, verbose=1, steps=X_test.shape[0])  # evaluate the model\n",
        "    print('Final loss 1 (cross-entropy and accuracy and F1):', loss)\n",
        "    y_pred = model.predict(X_test)\n",
        "    threshold = 0.3  # Define a threshold for binary classification (e.g., 0.5)\n",
        "    y_pred_binary = (y_pred >= threshold).astype(int)  # Convert predicted probabilities to binary labels\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    plot_confusion_matrix(cf_matrix)\n",
        "    # Calculate precision\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    # Calculate recall\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n"
      ],
      "metadata": {
        "id": "Wk7mMS_51f8m"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Models"
      ],
      "metadata": {
        "id": "GboEN0721oaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_model_with_1_hidden_layer(X_train, X_test, y_train, y_test, model_name, total_nodes=16, initial_learning_rate=0.001, num_epochs=100, opt='adam', threshold=0.5):\n",
        "\n",
        "    \"\"\"\n",
        "    Build, train, and evaluate an LSTM model with one hidden layer for binary classification.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (numpy.ndarray): Training input data with shape (samples, time steps, features).\n",
        "    - X_test (numpy.ndarray): Testing input data with shape (samples, time steps, features).\n",
        "    - y_train (numpy.ndarray): Training target data with binary labels.\n",
        "    - y_test (numpy.ndarray): Testing target data with binary labels.\n",
        "    - model_name (str): Name of the model used for saving checkpoints.\n",
        "    - total_nodes (int): Number of LSTM units in the hidden layer. Default is 16.\n",
        "    - initial_learning_rate (float): Initial learning rate for the optimizer. Default is 0.001.\n",
        "    - num_epochs (int): Number of training epochs. Default is 100.\n",
        "    - opt (str): String specifying the optimizer to be used. Default is 'adam'.\n",
        "    - threshold (float): Threshold for binary classification. Default is 0.5.\n",
        "\n",
        "    - The function constructs an LSTM model with one hidden layer, followed by a Dense layer with a sigmoid activation.\n",
        "    - It compiles and trains the model using the specified parameters and displays training history plots.\n",
        "    - Evaluates the model on the testing data and prints the final loss.\n",
        "    - Generates and displays a confusion matrix along with precision and recall scores.\n",
        "    \"\"\"\n",
        "\n",
        "    model = Sequential()\n",
        "    # input_shape - we define an LSTM model with an input shape of (param1, param2), meaning it takes in a sequence\n",
        "    # of param1 inputs with param2 feature each.\n",
        "    model.add(LSTM(total_nodes, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Flatten())\n",
        "    # The sigmoid activation function is commonly used for binary classification problems-\n",
        "    # where the output values range between 0 and 1\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    lr_schedule = callbacks_function(model_name)\n",
        "    model_history = model_comiple_run(num_epochs, initial_learning_rate, model, X_train, y_train, X_test, y_test,\n",
        "                                      callbacks=[lr_schedule], optimizer = opt)\n",
        "    model_plot(model_history)\n",
        "    loss = model.evaluate(X_test, y_test, verbose=1, steps=X_test.shape[0])  # evaluate the model\n",
        "    print('Final loss 1 (cross-entropy and accuracy and F1):', loss)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_binary = (y_pred >= threshold).astype(int)  # Convert predicted probabilities to binary labels\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    plot_confusion_matrix(cf_matrix)\n",
        "    # Calculate precision\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    # Calculate recall\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")"
      ],
      "metadata": {
        "id": "LoGadIME1ubJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_model_with_Batch_Normalization(X_train, X_test, y_train, y_test,model_name):\n",
        "\n",
        "    \"\"\"\n",
        "    Build, train, and evaluate an LSTM model with Batch Normalization for binary classification.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (numpy.ndarray): Training input data with shape (samples, time steps, features).\n",
        "    - X_test (numpy.ndarray): Testing input data with shape (samples, time steps, features).\n",
        "    - y_train (numpy.ndarray): Training target data with binary labels.\n",
        "    - y_test (numpy.ndarray): Testing target data with binary labels.\n",
        "    - model_name (str): Name of the model used for saving checkpoints.\n",
        "\n",
        "\n",
        "    - Constructs an LSTM model with Batch Normalization layers, enhancing training stability and convergence.\n",
        "    - Batch Normalization normalizes the input of each layer, reducing internal covariate shift during training.\n",
        "    - The model includes LSTM layers, Batch Normalization after each LSTM layer, a Flatten layer, and a Dense layer.\n",
        "    - Compiles and trains the model using specified parameters, and displays training history plots.\n",
        "    - Evaluates the model on testing data, printing the final loss, and generates a confusion matrix.\n",
        "    - Computes precision and recall scores based on the binary classification results.\n",
        "    \"\"\"\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = Sequential()\n",
        "    # input_shape - we define an LSTM model with an input shape of (param1, param2), meaning it takes in a sequence\n",
        "    # of param1 inputs with param2 feature each.\n",
        "    model.add(LSTM(16, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LSTM(16, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    model_name = 'Model2'\n",
        "    lr_schedule = callbacks_function(model_name)\n",
        "    initial_learning_rate = 0.01\n",
        "    num_epochs = 50\n",
        "    model_history = model_comiple_run(num_epochs, initial_learning_rate, model, X_train, y_train, X_test, y_test,\n",
        "                                      callbacks=[lr_schedule], optimizer=\"Adagrade\")\n",
        "    model_plot(model_history)\n",
        "    loss = model.evaluate(X_test, y_test, verbose=1, steps=X_test.shape[0])  # evaluate the model\n",
        "    print('Final loss 1 (cross-entropy and accuracy and F1):', loss)\n",
        "    y_pred = model.predict(X_test)\n",
        "    threshold = 0.5\n",
        "    y_pred_binary = (y_pred >= threshold).astype(int)  # Convert predicted probabilities to binary labels\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    plot_confusion_matrix(cf_matrix)\n",
        "    # Calculate precision & recall\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")"
      ],
      "metadata": {
        "id": "WjGhZxmsObxD"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_model_with_2_hidden_layer(X_train, X_test, y_train, y_test, model_name, total_nodes = 16,initial_learning_rate = 0.001, num_epochs = 100, opt='adam', threshold = 0.5):\n",
        "\n",
        "    \"\"\"\n",
        "    Build, train, and evaluate an LSTM model with two hidden layer for binary classification.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (numpy.ndarray): Training input data with shape (samples, time steps, features).\n",
        "    - X_test (numpy.ndarray): Testing input data with shape (samples, time steps, features).\n",
        "    - y_train (numpy.ndarray): Training target data with binary labels.\n",
        "    - y_test (numpy.ndarray): Testing target data with binary labels.\n",
        "    - model_name (str): Name of the model used for saving checkpoints.\n",
        "    - total_nodes (int): Number of LSTM units in the hidden layer. Default is 16.\n",
        "    - initial_learning_rate (float): Initial learning rate for the optimizer. Default is 0.001.\n",
        "    - num_epochs (int): Number of training epochs. Default is 100.\n",
        "    - opt (str): String specifying the optimizer to be used. Default is 'adam'.\n",
        "    - threshold (float): Threshold for binary classification. Default is 0.5.\n",
        "\n",
        "    - The function constructs an LSTM model with two hidden layer, followed by a Dense layer with a sigmoid activation.\n",
        "    - It compiles and trains the model using the specified parameters and displays training history plots.\n",
        "    - Evaluates the model on the testing data and prints the final loss.\n",
        "    - Generates and displays a confusion matrix along with precision and recall scores.\n",
        "    \"\"\"\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = Sequential()\n",
        "    # input_shape - we define an LSTM model with an input shape of (param1, param2), meaning it takes in a sequence\n",
        "    # of param1 inputs with param2 feature each.\n",
        "    model.add(LSTM(total_nodes, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "    model.add(LSTM(total_nodes, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "\n",
        "    lr_schedule = callbacks_function(model_name)\n",
        "    model_history = model_comiple_run(num_epochs, initial_learning_rate, model, X_train, y_train, X_test, y_test,\n",
        "                                      callbacks=[lr_schedule], optimizer = opt)\n",
        "    model_plot(model_history)\n",
        "    loss = model.evaluate(X_test, y_test, verbose=1, steps=X_test.shape[0])  # evaluate the model\n",
        "    print('Final loss 1 (cross-entropy and accuracy and F1):', loss)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_binary = (y_pred >= threshold).astype(int)  # Convert predicted probabilities to binary labels\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    plot_confusion_matrix(cf_matrix)\n",
        "    # Calculate precision & recall\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")"
      ],
      "metadata": {
        "id": "QaBXDLPZQgoF"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lstm_model_with_3_hidden_layer(X_train, X_test, y_train, y_test, model_name, total_nodes = 16, initial_learning_rate = 0.01, opt='Adagrade', num_epochs = 50):\n",
        "\n",
        "    \"\"\"\n",
        "    Build, train, and evaluate an LSTM model with three hidden layer for binary classification.\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (numpy.ndarray): Training input data with shape (samples, time steps, features).\n",
        "    - X_test (numpy.ndarray): Testing input data with shape (samples, time steps, features).\n",
        "    - y_train (numpy.ndarray): Training target data with binary labels.\n",
        "    - y_test (numpy.ndarray): Testing target data with binary labels.\n",
        "    - model_name (str): Name of the model used for saving checkpoints.\n",
        "    - total_nodes (int): Number of LSTM units in the hidden layer. Default is 16.\n",
        "    - initial_learning_rate (float): Initial learning rate for the optimizer. Default is 0.001.\n",
        "    - num_epochs (int): Number of training epochs. Default is 100.\n",
        "    - opt (str): String specifying the optimizer to be used. Default is 'adam'.\n",
        "    - threshold (float): Threshold for binary classification. Default is 0.5.\n",
        "\n",
        "    - The function constructs an LSTM model with three hidden layer, followed by a Dense layer with a sigmoid activation.\n",
        "    - It compiles and trains the model using the specified parameters and displays training history plots.\n",
        "    - Evaluates the model on the testing data and prints the final loss.\n",
        "    - Generates and displays a confusion matrix along with precision and recall scores.\n",
        "    \"\"\"\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    model = Sequential()\n",
        "    # input_shape - we define an LSTM model with an input shape of (param1, param2), meaning it takes in a sequence\n",
        "    # of param1 inputs with param2 feature each.\n",
        "    model.add(LSTM(total_nodes, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "    model.add(LSTM(total_nodes, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))\n",
        "    model.add(LSTM(total_nodes, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.summary()\n",
        "    lr_schedule = callbacks_function(model_name)\n",
        "    model_history = model_comiple_run(num_epochs, initial_learning_rate, model, X_train, y_train, X_test, y_test,\n",
        "                                      callbacks=[lr_schedule], optimizer=opt)\n",
        "    model_plot(model_history)\n",
        "    loss = model.evaluate(X_test, y_test, verbose=1, steps=X_test.shape[0])  # evaluate the model\n",
        "    print('Final loss 1 (cross-entropy and accuracy and F1):', loss)\n",
        "    y_pred = model.predict(X_test)\n",
        "    threshold = 0.5\n",
        "    y_pred_binary = (y_pred >= threshold).astype(int)  # Convert predicted probabilities to binary labels\n",
        "    cf_matrix = confusion_matrix(y_test, y_pred_binary)\n",
        "    plot_confusion_matrix(cf_matrix)\n",
        "    # Calculate precision & recall\n",
        "    precision = precision_score(y_test, y_pred_binary)\n",
        "    recall = recall_score(y_test, y_pred_binary)\n",
        "    print(f\"Precision: {precision}\")\n",
        "    print(f\"Recall: {recall}\")\n"
      ],
      "metadata": {
        "id": "zTbOtLfnSvoh"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###A call to the upload data function and the prepare data function"
      ],
      "metadata": {
        "id": "YEhK2baVXaUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    X,Y = upload_data()\n",
        "    X_normalized = min_max_normalization(X)\n",
        "    X_train, X_test, y_train, y_test = encoder(X_normalized, Y)"
      ],
      "metadata": {
        "id": "tS6-l1m6XUx0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f776d5f-2cf0-46a1-ccea-de3057acc5ac"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starts by upload  missile trajectories from Google Drive...\n",
            "    This is going to take a while....\n",
            "There are 2600 CSV files in the 'missile_data' folder.\n",
            "Upload finished successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Initial improvements to LSTM models\n"
      ],
      "metadata": {
        "id": "P9xrBiBsYvOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    # lstm_model01(X_train, X_test, y_train, y_test)\n",
        "    # lstm_model02(X_train, X_test, y_train, y_test)\n",
        "    # lstm_model03(X_train, X_test, y_train, y_test)\n",
        "    # lstm_model04(X_train, X_test, y_train, y_test)\n",
        "    # lstm_model05(X_train, X_test, y_train, y_test)\n",
        "    # lstm_model06(X_train, X_test, y_train, y_test)\n",
        "    # lstm_model07(X_train, X_test, y_train, y_test)\n",
        "    # lstm_model08(X_train, X_test, y_train, y_test)\n",
        "    # lstm_model09(X_train, X_test, y_train, y_test)\n",
        "    # lstm_model010(X_train, X_test, y_train, y_test)\n"
      ],
      "metadata": {
        "id": "9B9pNkowYf1d"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#The final LSTM models\n"
      ],
      "metadata": {
        "id": "ttmAORPnY1DX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    lstm_1 = lstm_model_with_1_hidden_layer(X_train, X_test, y_train, y_test, \"Model_1\", total_nodes = 16, initial_learning_rate = 0.001, num_epochs = 100 ,opt='adam')\n"
      ],
      "metadata": {
        "id": "zEBPsmIX4G10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8758a078-f0b8-4259-a2fe-7ac63e70a804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_1 (LSTM)               (None, 16)                1664      \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1681 (6.57 KB)\n",
            "Trainable params: 1681 (6.57 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/100\n",
            "65/65 [==============================] - 43s 629ms/step - loss: 0.5890 - accuracy: 0.7120 - f1: 0.0272 - val_loss: 0.4623 - val_accuracy: 0.7692 - val_f1: 0.0000e+00\n",
            "Epoch 2/100\n",
            "65/65 [==============================] - 39s 601ms/step - loss: 0.3757 - accuracy: 0.8380 - f1: 0.7784 - val_loss: 0.2918 - val_accuracy: 0.9327 - val_f1: 1.5813\n",
            "Epoch 3/100\n",
            "65/65 [==============================] - 38s 589ms/step - loss: 0.2854 - accuracy: 0.9144 - f1: 1.4536 - val_loss: 0.2596 - val_accuracy: 0.9365 - val_f1: 1.6109\n",
            "Epoch 4/100\n",
            "65/65 [==============================] - 53s 825ms/step - loss: 0.2600 - accuracy: 0.9274 - f1: 1.5455 - val_loss: 0.2436 - val_accuracy: 0.9365 - val_f1: 1.6109\n",
            "Epoch 5/100\n",
            "65/65 [==============================] - 39s 601ms/step - loss: 0.2568 - accuracy: 0.9269 - f1: 1.5412 - val_loss: 0.2411 - val_accuracy: 0.9365 - val_f1: 1.6429\n",
            "Epoch 6/100\n",
            "65/65 [==============================] - 38s 590ms/step - loss: 0.2370 - accuracy: 0.9264 - f1: 1.5623 - val_loss: 0.2339 - val_accuracy: 0.9308 - val_f1: 1.5686\n",
            "Epoch 7/100\n",
            "65/65 [==============================] - 38s 591ms/step - loss: 0.2380 - accuracy: 0.9240 - f1: 1.5442 - val_loss: 0.2231 - val_accuracy: 0.9365 - val_f1: 1.6154\n",
            "Epoch 8/100\n",
            "65/65 [==============================] - 40s 625ms/step - loss: 0.2269 - accuracy: 0.9288 - f1: 1.5906 - val_loss: 0.2219 - val_accuracy: 0.9346 - val_f1: 1.6053\n",
            "Epoch 9/100\n",
            "65/65 [==============================] - 42s 646ms/step - loss: 0.2233 - accuracy: 0.9264 - f1: 1.5604 - val_loss: 0.2235 - val_accuracy: 0.9365 - val_f1: 1.6109\n",
            "Epoch 10/100\n",
            "65/65 [==============================] - 41s 631ms/step - loss: 0.2207 - accuracy: 0.9312 - f1: 1.5614 - val_loss: 0.2235 - val_accuracy: 0.9365 - val_f1: 1.6065\n",
            "Epoch 11/100\n",
            "65/65 [==============================] - 45s 693ms/step - loss: 0.2239 - accuracy: 0.9298 - f1: 1.5812 - val_loss: 0.2210 - val_accuracy: 0.9346 - val_f1: 1.6053\n",
            "Epoch 12/100\n",
            "65/65 [==============================] - 40s 611ms/step - loss: 0.2179 - accuracy: 0.9327 - f1: 1.5698 - val_loss: 0.2185 - val_accuracy: 0.9346 - val_f1: 1.6053\n",
            "Epoch 13/100\n",
            "65/65 [==============================] - 41s 637ms/step - loss: 0.2221 - accuracy: 0.9303 - f1: 1.5455 - val_loss: 0.2246 - val_accuracy: 0.9365 - val_f1: 1.6006\n",
            "Epoch 14/100\n",
            "65/65 [==============================] - 38s 592ms/step - loss: 0.2193 - accuracy: 0.9308 - f1: 1.5585 - val_loss: 0.2164 - val_accuracy: 0.9365 - val_f1: 1.6109\n",
            "Epoch 15/100\n",
            "65/65 [==============================] - 42s 642ms/step - loss: 0.2197 - accuracy: 0.9298 - f1: 1.5769 - val_loss: 0.2182 - val_accuracy: 0.9365 - val_f1: 1.6356\n",
            "Epoch 16/100\n",
            "65/65 [==============================] - 41s 631ms/step - loss: 0.2188 - accuracy: 0.9298 - f1: 1.5815 - val_loss: 0.2191 - val_accuracy: 0.9385 - val_f1: 1.6457\n",
            "Epoch 17/100\n",
            "65/65 [==============================] - 41s 632ms/step - loss: 0.2191 - accuracy: 0.9274 - f1: 1.5650 - val_loss: 0.2190 - val_accuracy: 0.9365 - val_f1: 1.6109\n",
            "Epoch 18/100\n",
            "65/65 [==============================] - 41s 637ms/step - loss: 0.2204 - accuracy: 0.9284 - f1: 1.5813 - val_loss: 0.2264 - val_accuracy: 0.9308 - val_f1: 1.6237\n",
            "Epoch 19/100\n",
            "65/65 [==============================] - 41s 634ms/step - loss: 0.2202 - accuracy: 0.9250 - f1: 1.5629 - val_loss: 0.2205 - val_accuracy: 0.9385 - val_f1: 1.6145\n",
            "Epoch 20/100\n",
            "65/65 [==============================] - 39s 595ms/step - loss: 0.2350 - accuracy: 0.9240 - f1: 1.5380 - val_loss: 0.2596 - val_accuracy: 0.9192 - val_f1: 1.6085\n",
            "Epoch 21/100\n",
            "65/65 [==============================] - 41s 635ms/step - loss: 0.2265 - accuracy: 0.9317 - f1: 1.5826 - val_loss: 0.2190 - val_accuracy: 0.9365 - val_f1: 1.6109\n",
            "Epoch 22/100\n",
            "65/65 [==============================] - 41s 636ms/step - loss: 0.2127 - accuracy: 0.9322 - f1: 1.5629 - val_loss: 0.2158 - val_accuracy: 0.9365 - val_f1: 1.6356\n",
            "Epoch 23/100\n",
            "65/65 [==============================] - 41s 634ms/step - loss: 0.2151 - accuracy: 0.9298 - f1: 1.5650 - val_loss: 0.2143 - val_accuracy: 0.9365 - val_f1: 1.6052\n",
            "Epoch 24/100\n",
            "65/65 [==============================] - 46s 716ms/step - loss: 0.2138 - accuracy: 0.9308 - f1: 1.5686 - val_loss: 0.2207 - val_accuracy: 0.9385 - val_f1: 1.6457\n",
            "Epoch 25/100\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.2149 - accuracy: 0.9317 - f1: 1.5568"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    lstm_2 = lstm_model_with_Batch_Normalization(X_train, X_test, y_train, y_test, \"Model_2\")\n"
      ],
      "metadata": {
        "id": "nKQHUXZT1K2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    lstm_3 = lstm_model_with_2_hidden_layer(X_train, X_test, y_train, y_test, \"Model_3\", total_nodes = 128,initial_learning_rate = 0.01, num_epochs = 100, opt='Adagrade')\n"
      ],
      "metadata": {
        "id": "CJRILLUT1M2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    lstm_4 = lstm_model_with_1_hidden_layer(X_train, X_test, y_train, y_test, \"Model_4\", total_nodes = 32, initial_learning_rate = 0.001, num_epochs = 50 ,opt='adam')\n"
      ],
      "metadata": {
        "id": "oJf6pIRs1Q-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    lstm_5 = lstm_model_with_1_hidden_layer(X_train, X_test, y_train, y_test, \"Model_5\", total_nodes = 16, initial_learning_rate = 0.001, num_epochs = 50 ,opt='adam')\n"
      ],
      "metadata": {
        "id": "Citmm2G-1UHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    lstm_6 = lstm_model_with_3_hidden_layer(X_train, X_test, y_train, y_test, \"Model_6\", total_nodes = 16, initial_learning_rate = 0.01, num_epochs = 50 ,opt='Adagrade')\n"
      ],
      "metadata": {
        "id": "2yTd2PT71dOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    lstm_7 = lstm_model_with_2_hidden_layer(X_train, X_test, y_train, y_test, \"Model_7\", total_nodes = 16,initial_learning_rate = 0.01, num_epochs = 50, opt='Adagrade')"
      ],
      "metadata": {
        "id": "szw0STp21Ww0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}